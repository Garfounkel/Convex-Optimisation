{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Descentes de gradient sans contraintes\n",
        "-------------------------------------------\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-1) Soit la famille de fonction: $f(x, y) = (1/2)x² + (1/2)ay² \\pourtout a \\in [1; +\\inf]$\n",
        "\n",
        "Le nombre de conditionnement d'un fonction de cette famille est équivalent au nombre de conditionnement de sa Hessienne:\n",
        "\n",
        "$H = \\begin{pmatrix} 1 & 0 \\\\ 0 & a \\end{pmatrix}$\n",
        "\n",
        "On choisit d'utiliser la norme $l2$ pour calculer le nombre de conditionnement de notre matrice.\n",
        "Dans ce cadre, le nombre de conditionnement est égale à: $||H||_2 * ||H^{-1}||_2$\n",
        "\nDans notre cas, le nombre de conditionnement correspond à la plus grande valeur singulière divisée par la plus petite. Donc le nombre de conditionnement est $a/1 = a$ avec $a \\in [1; +\\inf]$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_obj(a):\n",
        "    def obj_func(x):\n",
        "        return (x[0] ** 2)/2 + a*(x[1] ** 2)/2\n",
        "    return obj_func\n",
        "\n\n",
        "def build_derivative_obj(a):\n",
        "    def derivative_obj(x):\n",
        "        return x[0] + a*x[1]\n",
        "    return derivative_obj\n",
        "\n",
        "def gradient_descent(x, ob_function, d_direction, \n",
        "                     rate=(lambda x, y: 0.01),\n",
        "                     decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                     tolerance=0.0001, max_iter=1000,\n",
        "                     plot_f=(lambda x, y: None)):\n",
        "    \"\"\"Gradient Descent.\n",
        "    \n",
        "    Computes minimal value of a convex function and local minimum of none convex function.\n",
        "    \n",
        "    Args:\n",
        "        x (ndarray): initial starting point for descent.\n",
        "        ob_function: objective function of optimisation problem, has input ndarray and outputs float.\n",
        "        d_direction: function computing descent direction, outputs ndarray.\n",
        "        rate: function computing learning rate, outputs float.\n",
        "        decay_function: function computing decay, outputs float.\n",
        "        tolerance (float): slack tolerance.\n",
        "        max_iter (int): upper bound on number of iterations.      \n",
        "        plot_f: plotting function for iteration points.\n",
        "         \n",
        "    Output:\n",
        "        (int, int) minimizer, minimal value.\n",
        "        \n",
        "    \"\"\"\n",
        "    n_iter = 0\n",
        "    decay = tolerance + 10  # Make sure that we get into first loop\n",
        "    y = ob_function(x)\n",
        "    while decay > tolerance and n_iter < max_iter:\n",
        "        x = x - rate(ob_function, n_iter) * d_direction(x)\n",
        "        tmp_y = ob_function(x)\n",
        "        decay = decay_function(y, tmp_y)\n",
        "        y = tmp_y\n",
        "        n_iter += 1\n",
        "        plot_f(x, y)\n",
        "    print(' Iteration nu. = {}\\n approx. = {}\\n ob value = {}\\n and decay = {}.'.format(n_iter, x.flatten(), y[0], decay))\n",
        "    return (x, y) if decay <= tolerance else warnings.warn(\"Decay didn't get under tolerance rate.\", RuntimeWarning)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for a in range(1, 10):\n",
        "    m = np.array([[1, 0], [0, a]])\n",
        "    nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "    print(nb_cond1)\n",
        "    \n",
        "    obj = build_obj(a)\n",
        "    derivative_obj = build_derivative_obj(a)\n",
        "\n",
        "    start = np.array([[1], [1]], dtype=float)\n",
        "    gradient_descent(start, obj, derivative_obj, \n",
        "                     rate=(lambda x, y: 0.01),\n",
        "                     decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                     tolerance=0.0001, max_iter=1000,\n",
        "                     plot_f=(lambda x, y: None))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            " Iteration nu. = 150\n",
            " approx. = [0.04829602 0.04829602]\n",
            " ob value = 0.0023325056679514237\n",
            " and decay = 9.617578555901366e-05.\n",
            "2.0\n",
            " Iteration nu. = 113\n",
            " approx. = [0.03200413 0.03200413]\n",
            " ob value = 0.0015363961252909814\n",
            " and decay = 9.650442236656086e-05.\n",
            "3.0\n",
            " Iteration nu. = 92\n",
            " approx. = [0.02338587 0.02338587]\n",
            " ob value = 0.0010937973703817414\n",
            " and decay = 9.304873463316906e-05.\n",
            "4.0\n",
            " Iteration nu. = 78\n",
            " approx. = [0.01829958 0.01829958]\n",
            " ob value = 0.0008371869186920438\n",
            " and decay = 9.04440161467858e-05.\n",
            "5.0\n",
            " Iteration nu. = 67\n",
            " approx. = [0.01583311 0.01583311]\n",
            " ob value = 0.000752061985747571\n",
            " and decay = 9.90719954063118e-05.\n",
            "6.0\n",
            " Iteration nu. = 60\n",
            " approx. = [0.01285218 0.01285218]\n",
            " ob value = 0.0005781251607261142\n",
            " and decay = 9.030490139218185e-05.\n",
            "7.0\n",
            " Iteration nu. = 54\n",
            " approx. = [0.01108007 0.01108007]\n",
            " ob value = 0.0004910721950103534\n",
            " and decay = 8.911707130622665e-05.\n",
            "8.0\n",
            " Iteration nu. = 49\n",
            " approx. = [0.00984075 0.00984075]\n",
            " ob value = 0.00043578167240167833\n",
            " and decay = 9.046113933806123e-05.\n",
            "9.0\n",
            " Iteration nu. = 45\n",
            " approx. = [0.00872796 0.00872796]\n",
            " ob value = 0.00038088674022933186\n",
            " and decay = 8.93438032636704e-05.\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tensorflow"
    },
    "kernelspec": {
      "name": "tensorflow",
      "language": "python",
      "display_name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "version": "3.5.2",
      "name": "python",
      "nbconvert_exporter": "python",
      "mimetype": "text/x-python",
      "pygments_lexer": "ipython3"
    },
    "nteract": {
      "version": "0.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}