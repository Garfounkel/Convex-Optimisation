{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Descentes de gradient sans contraintes\n",
        "-------------------------------------------\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-1) Soit la famille de fonction: $f(x, y) = \\frac{x^2}{2} + \\frac{ay^2}{2} ~\\forall~ a \\in [1; +\\inf]$\n",
        "\n",
        "Le nombre de conditionnement d'une fonction de cette famille est équivalent au nombre de conditionnement de sa Hessienne:\n",
        "\n",
        "$H = \\begin{pmatrix} 1 & 0 \\\\ 0 & a \\end{pmatrix}$\n",
        "\n",
        "On choisit d'utiliser la norme $l_2$ pour calculer le nombre de conditionnement de notre matrice.\n",
        "Dans ce cadre, le nombre de conditionnement est égal à: $||H||_2 \\times ||H^{-1}||_2$\n",
        "\nDans notre cas, le nombre de conditionnement correspond à la plus grande valeur singulière divisée par la plus petite. Donc le nombre de conditionnement est $\\frac{a}{1} = a$ avec $a \\in [1; +\\inf[$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy import misc"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_derivative(ob_function, point, var=0):\n",
        "    args = point\n",
        "    def wraps(x):\n",
        "        args[var] = x\n",
        "        return ob_function(args)\n",
        "    return misc.derivative(wraps, point[var], dx=1e-6)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def d_gd(ob_function, x):\n",
        "    dim = x.shape[0]\n",
        "    gradient = partial_derivative(ob_function, x, 0)\n",
        "    for i in range(1, dim):\n",
        "        gradient = np.vstack((gradient, partial_derivative(ob_function, x, i)))\n",
        "    return gradient\n",
        "\n",
        "def d_sgd(ob_function, x):\n",
        "    dim = x.shape[0]\n",
        "    sdescent = np.array([0]*dim, dtype=float).reshape(-1, 1)\n",
        "    max_diff, imax_diff = sdescent[0], 0\n",
        "    for i in range(dim):\n",
        "        diff_i = np.linalg.norm(partial_derivative(ob_function, x, i), 2)\n",
        "        if  diff_i > max_diff:\n",
        "            imax_diff, max_diff = i, diff_i\n",
        "    sdescent[imax_diff] = max_diff\n",
        "    return sdescent"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def build_obj(a):\n",
        "    def obj_func(x):\n",
        "        return (x[0] ** 2)/2 + a*(x[1] ** 2)/2\n",
        "    return obj_func\n",
        "\n",
        "def gradient_descent(x, ob_function, d_direction, \n",
        "                     rate=(lambda x, y: 0.01),\n",
        "                     decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                     tolerance=0.0001, max_iter=1000,\n",
        "                     plot_f=(lambda x, y: None),\n",
        "                     verbose=False,\n",
        "                     norm=2):\n",
        "    \"\"\"Gradient Descent.\n",
        "    \n",
        "    Computes minimal value of a convex function and local minimum of none convex function.\n",
        "    \n",
        "    Args:\n",
        "        x (ndarray): initial starting point for descent.\n",
        "        ob_function: objective function of optimisation problem, has input ndarray and outputs float.\n",
        "        d_direction: function computing descent direction, outputs ndarray.\n",
        "        rate: function computing learning rate, outputs float.\n",
        "        decay_function: function computing decay, outputs float.\n",
        "        tolerance (float): slack tolerance.\n",
        "        max_iter (int): upper bound on number of iterations.      \n",
        "        plot_f: plotting function for iteration points.\n",
        "         \n",
        "    Output:\n",
        "        (int, int) minimizer, minimal value.\n",
        "        \n",
        "    \"\"\"\n",
        "    n_iter = 0\n",
        "    decay = tolerance + 1\n",
        "    y = ob_function(x)\n",
        "    plot_f(x, y)\n",
        "    while decay > tolerance and n_iter < max_iter:\n",
        "        grad = d_direction(ob_function, x)\n",
        "        x = x - rate(ob_function, n_iter) * grad\n",
        "        decay = decay_function(grad[0], grad[1])\n",
        "        n_iter += 1\n",
        "        y = ob_function(x)\n",
        "        plot_f(x, y)\n",
        "    if verbose:\n",
        "        print(' Iteration nu. = {}\\n approx. = {}\\n ob value = {}\\n and decay = {}.'.format(n_iter, x.flatten(), y[0], decay))\n",
        "    if decay > tolerance:\n",
        "        warnings.warn(\"Decay didn't get under tolerance rate.\", RuntimeWarning)\n",
        "    return ((x, y), n_iter)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_nb_iterations(step, k=0, set_size=60, max_value=50, start=np.array([[1], [1]], dtype=float)):\n",
        "\n",
        "    a_values = np.linspace(1, max_value, num=set_size)\n",
        "    iterations = np.zeros((set_size, 1))\n",
        "    conditioning = np.zeros((set_size, 1))\n",
        "    results = np.zeros((set_size, 1))\n",
        "\n",
        "    count = 0\n",
        "    for a in a_values:\n",
        "        m = np.array([[1, 0], [0, a]])\n",
        "        nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "        conditioning[count, 0] = nb_cond1\n",
        "    \n",
        "        obj = build_obj(a)\n",
        "\n",
        "        (x, y), n_iter = gradient_descent(start, obj, d_sgd, \n",
        "                                     rate=(lambda x, y: step),\n",
        "                                     tolerance=0.0001, max_iter=1000,\n",
        "                                     plot_f=(lambda x, y: None))\n",
        "        iterations[count, 0] = n_iter\n",
        "        results[count, 0] = y[0]\n",
        "        count+=1\n",
        "\n",
        "    plt.plot(results, 'g', iterations, 'r', conditioning, 'b')\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pour un pas de 0.1**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations(0.1, set_size=50, max_value=20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "On remarque deux choses pour notre famille:\n",
        "- Le nombre d'itérations de la descente de gradient diminue quand le nombre de conditionnement augmente.\n",
        "- A partir d'un certain nombre de conditionnement limite, la descente de gradient diverge.\n",
        "\nVoici un exemple de divergence de l'algorithme pour les nombres de conditionnement trop élevés. Les valeurs objectives, tracées en vert, sont très éloignée de 0."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations(0.1, set_size=50, max_value=30)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "On effectue maintenant des tests pour plusieurs pas différents:\n",
        "\n**Pour un pas de 0.15:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations(0.15, set_size=50, max_value=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pour un pas de 0.05:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations(0.05, set_size=50, max_value=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pour un pas de 0.035:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations(0.035, set_size=50, max_value=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "a = 3\n",
        "m = np.array([[1, 0], [0, a]])\n",
        "nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "\n",
        "obj = build_obj(a)\n",
        "\n",
        "start = np.array([[1], [1]], dtype=float)\n",
        "P, n_iter = gradient_descent(start, obj, d_sgd, \n",
        "                             rate=(lambda x, y: 0.15),\n",
        "                             decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                             tolerance=0.0001, max_iter=1000,\n",
        "                             plot_f=(lambda x, y: None),\n",
        "                             verbose=True)\n",
        "\n",
        "a = 8\n",
        "m = np.array([[1, 0], [0, a]])\n",
        "nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "\n",
        "obj = build_obj(a)\n",
        "\n",
        "start = np.array([[1], [1]], dtype=float)\n",
        "P, n_iter = gradient_descent(start, obj, d_sgd, \n",
        "                             rate=(lambda x, y: 0.15),\n",
        "                             decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                             tolerance=0.0001, max_iter=1000,\n",
        "                             plot_f=(lambda x, y: None),\n",
        "                             verbose=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "On remarque que la précision des valeurs trouvées est sensiblement la même pour des nombres de conditionnement identiques. Cependant, choisir un nombre de conditionnement plus grand permet de choisir un step plus faible sans impacter trop fortement les performances. De cette manière, il est peut-être possible d'augmenter la précision de la valeur objective finale."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-2) Comparaison nomres L1 - L2 **"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_nb_iterations_l1_l2(step, set_size=60, max_value=50):\n",
        "\n",
        "    a_values = np.linspace(1, max_value, num=set_size)\n",
        "    iterations = np.zeros((set_size, 1))\n",
        "    iterations2 = np.zeros((set_size, 1))\n",
        "    conditioning = np.zeros((set_size, 1))\n",
        "\n",
        "    count = 0\n",
        "    for a in a_values:\n",
        "        m = np.array([[1, 0], [0, a]])\n",
        "        nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "        conditioning[count, 0] = nb_cond1\n",
        "    \n",
        "        obj = build_obj(a)\n",
        "        \n",
        "        def norm2(x, y):\n",
        "            v = np.zeros((2, 1))\n",
        "            v[0, 0] = x\n",
        "            v[1, 0] = y\n",
        "            return np.linalg.norm(v, 2)\n",
        "\n",
        "        start = np.array([[1], [1]], dtype=float)\n",
        "        P, n_iter = gradient_descent(start, obj, d_sgd, \n",
        "                                     rate=(lambda x, y: step),\n",
        "                                     decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                                     tolerance=0.0001, max_iter=1000,\n",
        "                                     plot_f=(lambda x, y: None))\n",
        "        P2, n_iter2 = gradient_descent(start, obj, d_sgd, \n",
        "                                     rate=(lambda x, y: step),\n",
        "                                     decay_function=norm2,\n",
        "                                     tolerance=0.0001, max_iter=1000,\n",
        "                                     plot_f=(lambda x, y: None))\n",
        "        iterations[count, 0] = n_iter\n",
        "        iterations2[count, 0] = n_iter2\n",
        "        count+=1\n",
        "\n",
        "    # red = L1-norm\n",
        "    # green = L2-norm\n",
        "    plt.plot(iterations, 'r', iterations2, 'g', conditioning, 'b')\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations_l1_l2(0.04, 50, 40)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations_l1_l2(0.05, 20, 30)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-3) learning rate scheduling**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le but ici est de proposer une méthode pour faire évoluer le rate. Dans notre cas, nous allons implémenter un rate décroissant en fonction du nombre d'itérations. Cette méthode devrait permettre de choisir un taux plus fort et donc de converger plus vite. Elle introduit cependant le risque de réduire le rate trop rapidement.\n",
        "\n$r_0$ et $k$ sont des hyperparamètres pour cet algorithme."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_2d(ob_function, xmin=-4, xmax=4, ymin=-2, ymax=2, nb_pts_h=50, nb_pts_v=50):\n",
        "    x1 = np.linspace(xmin, xmax, nb_pts_h)\n",
        "    x2 = np.linspace(ymin, ymax, nb_pts_v)\n",
        "    X, Y = np.meshgrid(x1, x2)\n",
        "    Z = ob_function(np.stack((X,Y), axis=0))\n",
        "    plt.contour(X, Y, Z, cmap='RdGy')\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plot_2d(obj)\n",
        "\n",
        "step = 0.3\n",
        "k = 0.015\n",
        "\n",
        "start=np.array([[-2], [1.5]], dtype=float)\n",
        "a = 8\n",
        "\n",
        "m = np.array([[1, 0], [0, a]])\n",
        "nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "\n",
        "obj = build_obj(a)\n",
        "\n",
        "(x, y), n_iter = gradient_descent(start, obj, d_gd, \n",
        "                                  rate=(lambda x, y: step * np.exp(-k * y)),\n",
        "                                  tolerance=0.0001, max_iter=1000,\n",
        "                                  plot_f=(lambda x, y:plt.scatter(x[0], x[1], c='k')))\n",
        "result = y\n",
        "\n",
        "print(\"result: \" + str(result))\n",
        "print(\"start step: \" + str(step))\n",
        "print(\"iterations: \" + str(n_iter))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations(0.3, set_size=50, max_value=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut remarquer que pour un rate de départ de 0.3, avec des nombres de conditionnement supérieurs à environ 7, la descente sans learning rate scheduling diverge. Le learning rate scheduling permet de corriger cet état de fait."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-4) descentes de gradient avancées**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descente accélérée de Nesterov:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gradient_descent(x, ob_function, d_direction, \n",
        "                     rate=(lambda x, y: 0.01),\n",
        "                     decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                     tolerance=0.0001, max_iter=1000,\n",
        "                     plot_f=(lambda x, y: None),\n",
        "                     verbose=False):\n",
        "    n_iter = 0\n",
        "    decay = tolerance + 10  # Make sure that we get into first loop\n",
        "    \n",
        "    y = ob_function(x)\n",
        "    y_t = x\n",
        "    t = 1\n",
        "    \n",
        "    while decay > tolerance and n_iter < max_iter:\n",
        "        curr_x = x - rate(ob_function, n_iter) * d_direction(ob_function, y_t)\n",
        "        curr_t = (1 + np.sqrt(1 + 4 * t**2)) / 2\n",
        "        curr_y = curr_x + ((t - 1)/curr_t) * (x - curr_x)\n",
        "        \n",
        "        y_t = curr_y\n",
        "        x = curr_x\n",
        "        t = curr_t\n",
        "        \n",
        "        tmp_y = ob_function(x)\n",
        "        decay = decay_function(y, tmp_y)\n",
        "        y = tmp_y\n",
        "        n_iter += 1\n",
        "        plot_f(x, y)\n",
        "    if verbose:\n",
        "        print(' Iteration nu. = {}\\n approx. = {}\\n ob value = {}\\n and decay = {}.'.format(n_iter, x.flatten(), y[0], decay))\n",
        "    return ((x, y), n_iter) if decay <= tolerance else warnings.warn(\"Decay didn't get under tolerance rate.\", RuntimeWarning)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_nb_iterations_nesterov(step, set_size=60, max_value=50):\n",
        "\n",
        "    a_values = np.linspace(1, max_value, num=set_size)\n",
        "    iterations = np.zeros((set_size, 1))\n",
        "    iterations2 = np.zeros((set_size, 1))\n",
        "    conditioning = np.zeros((set_size, 1))\n",
        "\n",
        "    count = 0\n",
        "    for a in a_values:\n",
        "        m = np.array([[1, 0], [0, a]])\n",
        "        nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "        conditioning[count, 0] = nb_cond1\n",
        "    \n",
        "        obj = build_obj(a)\n",
        "\n",
        "        start = np.array([[1], [1]], dtype=float)\n",
        "        P, n_iter = nesterov_gradient_descent(start, obj, d_sgd, \n",
        "                                     rate=(lambda x, y: step),\n",
        "                                     decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                                     tolerance=0.0001, max_iter=1000,\n",
        "                                     plot_f=(lambda x, y: None))\n",
        "        P2, n_iter2 = gradient_descent(start, obj, d_sgd, \n",
        "                                     rate=(lambda x, y: step),\n",
        "                                     decay_function=(lambda x, y: abs(x[0] - y[0])),\n",
        "                                     tolerance=0.0001, max_iter=1000,\n",
        "                                     plot_f=(lambda x, y: None))\n",
        "        iterations[count, 0] = n_iter\n",
        "        iterations2[count, 0] = n_iter2\n",
        "        count+=1\n",
        "\n",
        "    # red = nesterov\n",
        "    # green = classic\n",
        "    plt.plot(iterations, 'r', conditioning, 'b', iterations2, 'g')\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations_nesterov(0.1, 15, 10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations_nesterov(0.15, 15, 10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations_nesterov(0.2, 15, 10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "On montre que la descente de gradient de Nesterov permet de diminuer sensiblement le nombre d'itérations."
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(theta_0, obj_function, d_direction, learning_rate=0.001, b1=0.9, b2=0.999, epsilon=1e-8, max_iter=6000,\n",
        "         plot_f=(lambda x, y: None)):\n",
        "    m_t = 0\n",
        "    v_t = 0\n",
        "    t = 0\n",
        "    alpha = learning_rate\n",
        "    theta_t = theta_0\n",
        "\n",
        "    for n_iter in range(max_iter):\n",
        "        t += 1\n",
        "        grad_t = d_direction(obj_function, theta_t)\n",
        "        m_t = b1 * m_t + (1 - b1) * grad_t\n",
        "        v_t = b2 * v_t + (1 - b2) * (grad_t ** 2)\n",
        "        m_hat = m_t / (1 - (b1 ** t))\n",
        "        v_hat = v_t / (1 - (b2 ** t))\n",
        "        theta_t_prev = theta_t\n",
        "        theta_t = theta_t - (alpha * m_hat) / (np.sqrt(v_hat) - epsilon)\n",
        "        y = obj_function(theta_t)\n",
        "\n",
        "        plot_f(theta_t, y)\n",
        "\n",
        "        if np.linalg.norm(grad_t) < epsilon:\n",
        "            break\n",
        "\n    return ((x, y), n_iter) if n_iter < max_iter else warnings.warn(\"Stopped at max_iter.\", RuntimeWarning)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_nb_iterations_adam(step, set_size=60, max_value=50):\n",
        "\n",
        "    a_values = np.linspace(1, max_value, num=set_size)\n",
        "    iterations = np.zeros((set_size, 1))\n",
        "    iterations2 = np.zeros((set_size, 1))\n",
        "    conditioning = np.zeros((set_size, 1))\n",
        "\n",
        "    count = 0\n",
        "    for a in a_values:\n",
        "        m = np.array([[1, 0], [0, a]])\n",
        "        nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "        conditioning[count, 0] = nb_cond1\n",
        "\n",
        "        obj = build_obj(a)\n",
        "\n",
        "        start = np.array([[1], [1]], dtype=float)\n",
        "        P, n_iter = adam(start, obj, d_sgd)\n",
        "        P2, n_iter2 = gradient_descent(start, obj, d_sgd,\n",
        "                                       rate=(lambda x, y: step),\n",
        "                                       decay_function=(\n",
        "                                           lambda x, y: abs(x[0] - y[0])),\n",
        "                                       tolerance=0.0001, max_iter=1000,\n",
        "                                       plot_f=(lambda x, y: None))\n",
        "        iterations[count, 0] = n_iter\n",
        "        iterations2[count, 0] = n_iter2\n",
        "        count += 1\n",
        "\n",
        "    # red = adam\n",
        "    # green = classic\n",
        "    plt.plot(iterations, 'r', conditioning, 'b', iterations2, 'g')\n",
        "    plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_nb_iterations_adam(0.15, 15, 10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plot_2d(obj)\n",
        "\n",
        "step = 0.3\n",
        "k = 0.015\n",
        "\n",
        "start=np.array([[-2], [1.5]], dtype=float)\n",
        "a = 8\n",
        "\n",
        "m = np.array([[1, 0], [0, a]])\n",
        "nb_cond1 = np.linalg.norm(m, 2) * np.linalg.norm(np.linalg.inv(m), 2)\n",
        "\n",
        "obj = build_obj(a)\n",
        "\n",
        "(x, y), n_iter = adam(start, obj, d_gd, plot_f=(lambda x, y:plt.scatter(x[0], x[1], c='k')))\n",
        "result = y\n",
        "\n",
        "print(\"result: \" + str(result))\n",
        "print(\"start step: \" + str(step))\n",
        "print(\"iterations: \" + str(n_iter))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}